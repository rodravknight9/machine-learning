{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e5202ea",
      "metadata": {},
      "source": [
        "# Pre-Processing Data Example\n",
        "\n",
        "This notebook walks through common **data preprocessing** steps before building a model: loading data, **cleaning** (handling missing values), **encoding** categorical variables (label and one-hot encoding), train/test splitting, and **feature scaling** (standardization). The dataset has country, age, salary, and a binary target (e.g. purchased)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba7b301",
      "metadata": {},
      "source": [
        "## Imports\n",
        "\n",
        "We use **NumPy** for arrays, **Pandas** for loading and slicing the CSV, **Matplotlib** for plotting (available for later use), and **scikit-learn** for imputation, encoding, train/test split, and scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2e3bfaef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd4dafe3",
      "metadata": {},
      "source": [
        "## Load features (X)\n",
        "\n",
        "Read the CSV with `pd.read_csv`, then take all columns **except the last** as the feature matrix `X` using `iloc[:, :-1]`. `.values` converts the DataFrame to a NumPy array. You can see the result has country names, age, and salary (and some `nan` values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "863e99fe",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['France', 44.0, 72000.0],\n",
              "       ['Spain', 27.0, 48000.0],\n",
              "       ['Germany', 30.0, 54000.0],\n",
              "       ['Spain', 38.0, 61000.0],\n",
              "       ['Germany', 40.0, nan],\n",
              "       ['France', 35.0, 58000.0],\n",
              "       ['Spain', nan, 52000.0],\n",
              "       ['France', 48.0, 79000.0],\n",
              "       ['Germany', 50.0, 83000.0],\n",
              "       ['France', 37.0, 67000.0]], dtype=object)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e42db05",
      "metadata": {},
      "source": [
        "## Load target (Y)\n",
        "\n",
        "The **target** (dependent variable) is the last column, e.g. \"Purchased\" (Yes/No). We extract it with `iloc[:, 3]` and store it as `Y` for later encoding and train/test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83f6694b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y = dataset.iloc[:, 3].values\n",
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcd96d5",
      "metadata": {},
      "source": [
        "## Cleaning: Handling missing data\n",
        "\n",
        "**Technique: Imputation (mean strategy).** Missing values (`nan`) in numeric columns can break many models. Here we use scikit-learn's **SimpleImputer** with `strategy='mean'`: it fits the mean of each column (over non-missing values) and replaces missing values with that mean. We apply it only to the numeric columns (columns 1 and 2: age and salary), so `X[:, 1:3]` is fitted and transformed. This is a simple, common **cleaning** step for numeric data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "032067a8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['France', 44.0, 72000.0],\n",
              "       ['Spain', 27.0, 48000.0],\n",
              "       ['Germany', 30.0, 54000.0],\n",
              "       ['Spain', 38.0, 61000.0],\n",
              "       ['Germany', 40.0, 63777.77777777778],\n",
              "       ['France', 35.0, 58000.0],\n",
              "       ['Spain', 38.77777777777778, 52000.0],\n",
              "       ['France', 48.0, 79000.0],\n",
              "       ['Germany', 50.0, 83000.0],\n",
              "       ['France', 37.0, 67000.0]], dtype=object)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# handling missing data\n",
        "from sklearn.impute import SimpleImputer\n",
        "# mean means the value to replace\n",
        "# and axis 0 means column\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "imputer = imputer.fit(X[:, 1:3])\n",
        "# replace the values in X\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13e24f30",
      "metadata": {},
      "source": [
        "## Encoding: Categorical features (LabelEncoder)\n",
        "\n",
        "**Technique: Label encoding.** The first column (country) is categorical (France, Spain, Germany). **LabelEncoder** converts each category to an integer (e.g. France=0, Germany=1, Spain=2). This is useful for the **target** variable, but for **features** it implies an order (e.g. Germany > France) that may be wrong. The next step uses **one-hot encoding** for the country feature to avoid that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67ae6241",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 44.0 72000.0]\n",
            " [2 27.0 48000.0]\n",
            " [1 30.0 54000.0]\n",
            " [2 38.0 61000.0]\n",
            " [1 40.0 63777.77777777778]\n",
            " [0 35.0 58000.0]\n",
            " [2 38.77777777777778 52000.0]\n",
            " [0 48.0 79000.0]\n",
            " [1 50.0 83000.0]\n",
            " [0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "# categorical data\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "# better to use LabelEncoder for the dependent variable\n",
        "labelencoder_X = LabelEncoder()\n",
        "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
        "print(X)\n",
        "# here we need to find a different approach\n",
        "# France is not more than Spain or Germany"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c197e64b",
      "metadata": {},
      "source": [
        "## Encoding: One-hot encoding (dummy variables)\n",
        "\n",
        "**Technique: One-hot encoding.** To avoid implying an order between countries, we replace the single label-encoded column with **dummy variables**: one binary column per category (e.g. France, Germany, Spain). **OneHotEncoder** with `sparse_output=False` produces a matrix of 0/1 columns. We then replace the first column of `X` with these new columns using `np.column_stack` and keep the numeric columns (age, salary). This is standard for nominal categorical features in regression/classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e988ba0a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
              "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
              "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
              "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
              "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
              "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
              "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
              "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
              "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
              "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# One hot encoding means creating dummy variables\n",
        "onehotencoder = OneHotEncoder(categories='auto', sparse_output=False)\n",
        "country_encoded = onehotencoder.fit_transform(X[:, 0:1])\n",
        "X = np.column_stack((country_encoded, X[:, 1:]))\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1139ffc6",
      "metadata": {},
      "source": [
        "## Encoding: Target variable (Y)\n",
        "\n",
        "For a **binary target** (e.g. Yes/No), **LabelEncoder** is appropriate: it converts labels to 0 and 1. We fit and transform `Y` so that the model receives numeric targets (e.g. 0 = No, 1 = Yes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4642196d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_encoded = LabelEncoder()\n",
        "Y = label_encoded.fit_transform(Y)\n",
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d94faa4",
      "metadata": {},
      "source": [
        "## Train/test split\n",
        "\n",
        "**Technique: Holdout split.** We split the data into training and test sets with **train_test_split** so we can evaluate the model on unseen data. Here `test_size=0.2` keeps 20% for testing and 80% for training; `random_state=0` makes the split reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "04ab83b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7c4f52",
      "metadata": {},
      "source": [
        "## Inspect train and test sets\n",
        "\n",
        "Display `X_train` and `X_test` to confirm the split and that features (one-hot country, age, salary) look correct before scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a346d3b2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
              "       [1.0, 0.0, 0.0, 37.0, 67000.0],\n",
              "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
              "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
              "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
              "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
              "       [1.0, 0.0, 0.0, 44.0, 72000.0],\n",
              "       [1.0, 0.0, 0.0, 35.0, 58000.0]], dtype=object)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4a456167",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.0, 1.0, 0.0, 30.0, 54000.0],\n",
              "       [0.0, 1.0, 0.0, 50.0, 83000.0]], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d016ada",
      "metadata": {},
      "source": [
        "**Standardization** (z-score) formula:\n",
        "\n",
        "$x_{std} = \\frac{x - \\text{mean}(x)}{\\text{std}(x)}$\n",
        "\n",
        "Each feature is scaled to mean 0 and standard deviation 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4633424b",
      "metadata": {},
      "source": [
        "**Normalization** (min-max scaling) formula:\n",
        "\n",
        "$x_{norm} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$\n",
        "\n",
        "Values are scaled to the range [0, 1]. This notebook uses **StandardScaler** (standardization) below; use **MinMaxScaler** if you prefer this range."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d03f313",
      "metadata": {},
      "source": [
        "## Feature scaling: Standardization\n",
        "\n",
        "**Technique: Standardization (z-score).** Many algorithms (e.g. SVM, gradient-based methods) work better when features are on a similar scale. **StandardScaler** transforms each feature to have mean 0 and standard deviation 1 using the formula above. We **fit** the scaler on the **training set only** (`fit_transform(X_train)`), then **transform** the test set with the same learned mean and std (`transform(X_test)`). Never fit on the test set to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6c7f9a48",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.        ,  2.64575131, -0.77459667, -1.45882927, -0.90166297],\n",
              "       [-1.        ,  2.64575131, -0.77459667,  1.98496442,  2.13981082]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "X_train\n",
        "X_test"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
